# Training configuration for single GPU testing
# Usage: python scripts/train.py --config configs/training_single_gpu.yaml

model:
  model_name_or_path: "./qwen3-encdec-initialized"
  tokenizer_name_or_path: "./qwen3-encdec-initialized"
  num_sentinel_tokens: 100
  use_flash_attention: true

data:
  dataset_name: "wikimedia/wikipedia"
  dataset_config: "20231101.en"
  text_column: "text"
  streaming: true
  max_seq_length: 512
  max_encoder_length: 256
  max_decoder_length: 128
  ul2_task_weights: [1, 1, 1, 1, 4]
  preprocessing_num_workers: 2
  shuffle_buffer_size: 1000

training:
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 8

  learning_rate: 1.0e-4
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8
  max_grad_norm: 1.0

  num_train_steps: 1000
  warmup_steps: 100
  lr_scheduler_type: "cosine"

  bf16: true
  fp16: false

  # GPU Optimizations
  use_tf32: true
  use_liger_kernels: true
  use_cut_cross_entropy: true
  torch_compile: false  # Disabled - causes issues with dynamic shapes in RoPE
  use_fused_adamw: true  # Use CUDA fused AdamW (faster)

  save_steps: 500
  save_total_limit: 2

  eval_steps: 100
  eval_samples: 100

  logging_steps: 10
  report_to: ["wandb"]  # Enable wandb logging

  seed: 42

infra:
  output_dir: "./output"
  logging_dir: "./logs"
  distributed_type: "no"  # Single GPU
  fsdp_sharding_strategy: null
  fsdp_activation_checkpointing: false
  fsdp_cpu_offload: false
  deepspeed_config: null
  num_gpus: 1
  wandb_project: "qwen3-encoder-decoder"
  wandb_entity: null
  wandb_run_name: null
  wandb_watch: "gradients"  # Log gradient histograms
  wandb_log_model: false
  wandb_tags: ["qwen3-encdec", "single-gpu", "test"]
